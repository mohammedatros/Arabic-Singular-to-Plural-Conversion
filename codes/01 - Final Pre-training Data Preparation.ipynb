{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf_dUQL_lcOz"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tytDsTGlCs2q",
        "outputId": "978fc9ad-9f28-4639-9336-f25c61279945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyArabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from PyArabic) (1.15.0)\n",
            "Installing collected packages: PyArabic\n",
            "Successfully installed PyArabic-0.6.15\n"
          ]
        }
      ],
      "source": [
        "# install PyArabic library for Arabic preprocessing\n",
        "!pip install PyArabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB3hTHNrc_vN",
        "outputId": "069e8dd9-4b9e-4269-b628-1c1c03b52eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.1\n"
          ]
        }
      ],
      "source": [
        "# install tensorflow_addons for AdamW optimizer\n",
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6nYX2Lsfeki"
      },
      "outputs": [],
      "source": [
        "# Standard libraries imports\n",
        "import gc\n",
        "import io\n",
        "import re\n",
        "import ast\n",
        "import math\n",
        "import glob\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from lxml import etree\n",
        "import pyarabic.araby as araby\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRNuuzAIBuB_",
        "outputId": "1631caaf-166d-4a62-9e2e-f82668f3348a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_files = glob.glob('/content/drive/MyDrive/NLP_Course/New Corpus/sharded/*')\n",
        "train_files = []\n",
        "test_files = []\n",
        "for file in all_files:\n",
        "  if 'train' in file:\n",
        "    train_files.append(file)\n",
        "  elif 'test' in file:\n",
        "    test_files.append(file)"
      ],
      "metadata": {
        "id": "anvd-_2UA9Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full_test_corpus = ''\n",
        "# for file in tqdm(test_files, total=len(test_files)):\n",
        "#   with open(file, 'rb') as f:\n",
        "#     text = f.read().decode()\n",
        "#     full_test_corpus += ' ' + text\n",
        "#   del text\n",
        "#   gc.collect()\n",
        "\n",
        "# full_test_corpus = full_test_corpus.replace('\\n', ' ')\n",
        "# print('Number of words in the corpus:', len(full_test_corpus.split()))\n",
        "# print('Number of characters in the corpus:', len(full_test_corpus))\n",
        "# full_test_corpus = full_test_corpus.replace(' ', '#')\n",
        "\n",
        "# chars_to_remove = ['،', '.', ':', '؟', '/', '؛', '=']\n",
        "# for char in tqdm(chars_to_remove):\n",
        "#   full_test_corpus = full_test_corpus.replace(char, '#')"
      ],
      "metadata": {
        "id": "w0v4R2-nAo8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_unique_words = []\n",
        "for file in tqdm(test_files, total=len(test_files)):\n",
        "  with open(file, 'rb') as f:\n",
        "    text = f.read().decode()\n",
        "\n",
        "  text = text.replace('\\n', ' ')\n",
        "  text = text.replace(' ', '#')\n",
        "\n",
        "  chars_to_remove = ['،', '.', ':', '؟', '/', '؛', '=']\n",
        "  for char in chars_to_remove:\n",
        "    text = text.replace(char, '#')\n",
        "  text_as_list = list(set(text.split('#')))\n",
        "\n",
        "  all_unique_words.extend(text_as_list)\n",
        "  del text, text_as_list\n",
        "  gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkJMY9atUaGu",
        "outputId": "d2c03fa0-3a94-4da3-8d59-e1e9508bac7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [03:23<00:00,  1.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_unique_words = list(set(all_unique_words))"
      ],
      "metadata": {
        "id": "lDMWc8aKWau5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TY0XQcTWiGd",
        "outputId": "4508f088-8075-4167-9776-1ef4f4043381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1136706"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_unique_words[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe-jLLJIWlXF",
        "outputId": "87371ff9-3a8a-405e-bbd6-b12f07626407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'تسرقون',\n",
              " 'تساكن',\n",
              " 'أوترشت',\n",
              " 'للطيبرسي',\n",
              " 'توعدون',\n",
              " 'يرزء',\n",
              " 'وبالاتكال',\n",
              " 'وبالمروي',\n",
              " 'واستركبوه',\n",
              " 'ونزوءا',\n",
              " 'قفين',\n",
              " 'النخيب',\n",
              " 'وانحنائه',\n",
              " 'للترف',\n",
              " 'وسيضم',\n",
              " 'والمعانات',\n",
              " 'للطباع',\n",
              " 'ينجسك',\n",
              " 'كاتفاق',\n",
              " 'سيتكاثر',\n",
              " 'فيتحبب',\n",
              " 'وبطانيات',\n",
              " 'الطموية',\n",
              " 'الرقع',\n",
              " 'بخزازى',\n",
              " 'لرواية',\n",
              " 'إفينزا',\n",
              " 'املاه',\n",
              " 'وبمضمونه',\n",
              " 'كامانجار',\n",
              " 'فأفزعتها',\n",
              " 'بثعلب',\n",
              " 'إلدرسون',\n",
              " 'لأقتلهم',\n",
              " 'إجتهاد',\n",
              " 'القواديس',\n",
              " 'تاونزفيل',\n",
              " 'سجادة',\n",
              " 'إرضائها',\n",
              " 'والقيقاء',\n",
              " 'مألفا',\n",
              " 'ومنقطعات',\n",
              " 'اعجاب',\n",
              " 'مرائي',\n",
              " 'نفشوه',\n",
              " 'أصابعى',\n",
              " 'لعزكم',\n",
              " 'معرت',\n",
              " 'تغرينا']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "min_len = 100\n",
        "for word in final_unique_words:\n",
        "  if len(word) > max_len:\n",
        "    max_len = len(word)\n",
        "    longest_word = word\n",
        "  if len(word) < min_len:\n",
        "    min_len = len(word)\n",
        "    shortest_word = word\n",
        "  \n",
        "print('Max length:', max_len, 'Word is:', longest_word)\n",
        "print('Min length:', min_len, 'Word is:', shortest_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RLPxZ27W3Jw",
        "outputId": "b649fedd-b98a-4526-b3c8-ad18aed46892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 87 Word is: زماااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااااان\n",
            "Min length: 0 Word is: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exclude outliers\n",
        "max_len = 15\n",
        "min_len = 3\n",
        "for word in tqdm(final_unique_words):\n",
        "  if len(word) > max_len:\n",
        "    final_unique_words.remove(word)\n",
        "  if len(word) < min_len:\n",
        "    final_unique_words.remove(word)\n",
        "\n",
        "# detect new words    \n",
        "max_len = 0\n",
        "min_len = 100\n",
        "for word in final_unique_words:\n",
        "  if len(word) > max_len:\n",
        "    max_len = len(word)\n",
        "    longest_word = word\n",
        "  if len(word) < min_len:\n",
        "    min_len = len(word)\n",
        "    shortest_word = word\n",
        "  \n",
        "print('Max length:', max_len, 'Word is:', longest_word)\n",
        "print('Min length:', min_len, 'Word is:', shortest_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rm2oguOXf6z",
        "outputId": "3326f4a7-0fd9-4949-aa41-d34a0f0384f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1134950/1135892 [01:57<00:00, 9666.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 16 Word is: والحكميةوالشرعية\n",
            "Min length: 3 Word is: ذأم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text_as_list = full_test_corpus.split('#')\n",
        "# del full_test_corpus\n",
        "# gc.collect()\n",
        "# split_len = len(text_as_list)"
      ],
      "metadata": {
        "id": "INXsbSopXh5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_unique_words = []\n",
        "# for word in tqdm(text_as_list, total=split_len):\n",
        "#   if word not in all_unique_words and len(word) >= 5:\n",
        "#     all_unique_words.append(word)"
      ],
      "metadata": {
        "id": "KeHleICnT-Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_unique_chars = []\n",
        "# for char in tqdm(full_test_corpus, total=len(full_test_corpus)):\n",
        "#   if char not in all_unique_chars:\n",
        "#     all_unique_chars.append(char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2Eh_MAZHCYY",
        "outputId": "11e23ed6-cf8c-4343-8132-080ff1f02aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 410118332/410118332 [04:02<00:00, 1690429.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_unique_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUQHztJlIUH0",
        "outputId": "7a9a1813-1055-439a-b378-8a790b9e9f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#',\n",
              " 'أ',\n",
              " 'ف',\n",
              " 'ض',\n",
              " 'ل',\n",
              " 'ا',\n",
              " 'ص',\n",
              " 'ة',\n",
              " 'ب',\n",
              " 'ع',\n",
              " 'د',\n",
              " 'ر',\n",
              " 'ي',\n",
              " '،',\n",
              " 'و',\n",
              " 'ج',\n",
              " '.',\n",
              " 'ه',\n",
              " 'م',\n",
              " 'ث',\n",
              " 'ت',\n",
              " 'ذ',\n",
              " 'ك',\n",
              " 'ح',\n",
              " 'ى',\n",
              " 'ق',\n",
              " 'ط',\n",
              " 'س',\n",
              " ':',\n",
              " 'ش',\n",
              " 'ن',\n",
              " 'خ',\n",
              " 'غ',\n",
              " 'ظ',\n",
              " 'آ',\n",
              " 'إ',\n",
              " 'ء',\n",
              " 'ز',\n",
              " '؟',\n",
              " 'ئ',\n",
              " 'ؤ',\n",
              " '/',\n",
              " '؛',\n",
              " '=',\n",
              " '|']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final_unique_char = []\n",
        "# chars_to_remove = []\n",
        "# for char in all_unique_chars:\n",
        "#   print(char)\n",
        "#   add = input()\n",
        "#   if int(add) == 1:\n",
        "#      final_unique_char.append(char)\n",
        "#   else:\n",
        "#     chars_to_remove.append(char)\n",
        "# print(len(final_unique_char))\n",
        "# print(len(chars_to_remove))\n",
        "# print(chars_to_remove)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCtUcpKzIk8G",
        "outputId": "bd8e98a9-18f5-4f95-9155-43774ae170b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\n",
            "1\n",
            "أ\n",
            "1\n",
            "ف\n",
            "1\n",
            "ض\n",
            "1\n",
            "ل\n",
            "1\n",
            "ا\n",
            "1\n",
            "ص\n",
            "1\n",
            "ة\n",
            "1\n",
            "ب\n",
            "1\n",
            "ع\n",
            "1\n",
            "د\n",
            "1\n",
            "ر\n",
            "1\n",
            "ي\n",
            "1\n",
            "،\n",
            "0\n",
            "و\n",
            "1\n",
            "ج\n",
            "1\n",
            ".\n",
            "0\n",
            "ه\n",
            "1\n",
            "م\n",
            "1\n",
            "ث\n",
            "1\n",
            "ت\n",
            "1\n",
            "ذ\n",
            "1\n",
            "ك\n",
            "1\n",
            "ح\n",
            "1\n",
            "ى\n",
            "1\n",
            "ق\n",
            "1\n",
            "ط\n",
            "1\n",
            "س\n",
            "1\n",
            ":\n",
            "0\n",
            "ش\n",
            "1\n",
            "ن\n",
            "1\n",
            "خ\n",
            "1\n",
            "غ\n",
            "1\n",
            "ظ\n",
            "1\n",
            "آ\n",
            "1\n",
            "إ\n",
            "1\n",
            "ء\n",
            "1\n",
            "ز\n",
            "1\n",
            "؟\n",
            "0\n",
            "ئ\n",
            "1\n",
            "ؤ\n",
            "1\n",
            "/\n",
            "0\n",
            "؛\n",
            "0\n",
            "=\n",
            "0\n",
            "|\n",
            "1\n",
            "38\n",
            "7\n",
            "['،', '.', ':', '؟', '/', '؛', '=']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_unique_char = ['#',\n",
        "                      'أ',\n",
        "                      'ف',\n",
        "                      'ض',\n",
        "                      'ل',\n",
        "                      'ا',\n",
        "                      'ص',\n",
        "                      'ة',\n",
        "                      'ب',\n",
        "                      'ع',\n",
        "                      'د',\n",
        "                      'ر',\n",
        "                      'ي',\n",
        "                      'و',\n",
        "                      'ج',\n",
        "                      'ه',\n",
        "                      'م',\n",
        "                      'ث',\n",
        "                      'ت',\n",
        "                      'ذ',\n",
        "                      'ك',\n",
        "                      'ح',\n",
        "                      'ى',\n",
        "                      'ق',\n",
        "                      'ط',\n",
        "                      'س',\n",
        "                      'ش',\n",
        "                      'ن',\n",
        "                      'خ',\n",
        "                      'غ',\n",
        "                      'ظ',\n",
        "                      'آ',\n",
        "                      'إ',\n",
        "                      'ء',\n",
        "                      'ز',\n",
        "                      'ئ',\n",
        "                      'ؤ',\n",
        "                      '|']"
      ],
      "metadata": {
        "id": "JEX9Vigmb-Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars_to_remove = ['،', '.', ':', '؟', '/', '؛', '=']"
      ],
      "metadata": {
        "id": "ayc1asg0b71N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for char in tqdm(chars_to_remove):\n",
        "#   full_test_corpus = full_test_corpus.replace(char, '#')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGNdP_l_KDBM",
        "outputId": "15587787-f523-4238-9e05-2f80532de46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:04<00:00,  1.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final_unique_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZMr8OJIKq7v",
        "outputId": "a34352db-12b7-4fb9-d50a-4a07c3344b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#',\n",
              " 'أ',\n",
              " 'ف',\n",
              " 'ض',\n",
              " 'ل',\n",
              " 'ا',\n",
              " 'ص',\n",
              " 'ة',\n",
              " 'ب',\n",
              " 'ع',\n",
              " 'د',\n",
              " 'ر',\n",
              " 'ي',\n",
              " 'و',\n",
              " 'ج',\n",
              " 'ه',\n",
              " 'م',\n",
              " 'ث',\n",
              " 'ت',\n",
              " 'ذ',\n",
              " 'ك',\n",
              " 'ح',\n",
              " 'ى',\n",
              " 'ق',\n",
              " 'ط',\n",
              " 'س',\n",
              " 'ش',\n",
              " 'ن',\n",
              " 'خ',\n",
              " 'غ',\n",
              " 'ظ',\n",
              " 'آ',\n",
              " 'إ',\n",
              " 'ء',\n",
              " 'ز',\n",
              " 'ئ',\n",
              " 'ؤ',\n",
              " '|']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NOvELLKlinQ"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY2Le0HZf2w4"
      },
      "outputs": [],
      "source": [
        "# # configurations class\n",
        "# class config:\n",
        "\n",
        "#   MAXLEN = 128 # maximum length of sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzdOcFqOMUf5"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEb7_GE9V0f5",
        "outputId": "e5d3a11a-02c4-4775-e913-699d1756367c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Arabic Characters: 38\n"
          ]
        }
      ],
      "source": [
        "# list of all Arabic characters\n",
        "print('Number of Arabic Characters:', len(final_unique_char))\n",
        "\n",
        "# character to index dictionary\n",
        "char_to_index = dict((char, index+3) for (index, char) in enumerate(final_unique_char))\n",
        "# index to character dictionary\n",
        "index_to_char=  dict((index+3, char) for (index, char) in enumerate(final_unique_char))\n",
        "\n",
        "char_to_index['$'] = 0 # pad\n",
        "char_to_index['#'] = 1 # separator\n",
        "char_to_index['_'] = 2 # mask\n",
        "\n",
        "\n",
        "index_to_char[0] = '$' # pad\n",
        "index_to_char[1] = '#' # separator\n",
        "index_to_char[2] = '_' # mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QgwUeJB1x1P",
        "outputId": "9ece64c6-218f-45aa-ef1e-92f2ad080222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3204049/3204049 [00:06<00:00, 489158.34it/s]\n"
          ]
        }
      ],
      "source": [
        "# # convert the pure text into a list of sequences\n",
        "# text_as_list = []\n",
        "# for i in tqdm(range(0, len(full_test_corpus)-config.MAXLEN, 128)):\n",
        "#   text_as_list.append([full_test_corpus[i:i+config.MAXLEN]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFK6MAjtZfaY",
        "outputId": "5c27a8f8-0ac0-43b1-ab46-4e5f9a7d3b66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3204049/3204049 [01:36<00:00, 33260.82it/s]\n"
          ]
        }
      ],
      "source": [
        "# def text_to_index(lists):\n",
        "#     # construct a list that includes the character-based tokenized input words\n",
        "#     # for the encoder part\n",
        "#     bert_indexed_inputs = []\n",
        "    \n",
        "#     # iterate over the lists\n",
        "#     for text in tqdm(lists):\n",
        "#       indexes = []\n",
        "#       for char in list(text[0]):\n",
        "#         indexes.append(char_to_index[char]) # try to get the index from the first dictionary\n",
        "#       bert_indexed_inputs.append(indexes)\n",
        "#     return bert_indexed_inputs\n",
        "# # tokenize the dataset by relating each token to its index\n",
        "# indexed_text = text_to_index(text_as_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_index(text_list):\n",
        "    # construct a list that includes the character-based tokenized input words\n",
        "    # for the encoder part\n",
        "    bert_indexed_inputs = []\n",
        "    \n",
        "    # iterate over the lists\n",
        "    for word in tqdm(text_list):\n",
        "      word_as_list = ['#'] + list(word) + ['#'] # add '#' for the separator\n",
        "      indexes = []\n",
        "      for char in word_as_list:\n",
        "        indexes.append(char_to_index[char])\n",
        "      indexes.reverse()\n",
        "      bert_indexed_inputs.append(indexes)\n",
        "    return bert_indexed_inputs\n",
        "# tokenize the dataset by relating each token to its index\n",
        "indexed_text = text_to_index(final_unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf57AfWtZJwb",
        "outputId": "3e3e4e76-7e82-43cb-bede-54d455947a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1134950/1134950 [00:06<00:00, 181651.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvHF-sNb3uOZ"
      },
      "outputs": [],
      "source": [
        "# # train-validation-test split\n",
        "# # 90% for trianing, 5% for validation and 5% for test \n",
        "# train_len = int(0.9 * len(indexed_text))\n",
        "# train_data = indexed_text[:train_len]\n",
        "\n",
        "# valid_len = int(0.05 * len(indexed_text))\n",
        "# valid_data = indexed_text[train_len:train_len+valid_len]\n",
        "\n",
        "# test_data = indexed_text[train_len+valid_len:]\n",
        "\n",
        "# del indexed_text, text_as_list\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train-validation-test split\n",
        "# 90% for trianing, 5% for validation and 5% for test \n",
        "train_len = int(0.9 * len(indexed_text))\n",
        "train_data = indexed_text[:train_len]\n",
        "\n",
        "valid_len = int(0.05 * len(indexed_text))\n",
        "valid_data = indexed_text[train_len:train_len+valid_len]\n",
        "\n",
        "test_data = indexed_text[train_len+valid_len:]\n",
        "\n",
        "del indexed_text\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R91aB3ZdrZY",
        "outputId": "d7e72f19-eed5-406c-e872-f401c51e0dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(valid_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a5rw9LAM7cJ",
        "outputId": "32ee147b-7b94-4a27-9b0d-e8cc522ce9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1021455, 56747, 56748)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2l_2zksXEnW"
      },
      "outputs": [],
      "source": [
        "# def prepare_data(indexes):\n",
        "#   '''\n",
        "#   Args: list of characters in the form of indexes\n",
        "#   Returns: gold labels, masked labels, mask\n",
        "#   '''\n",
        "#   y_true = indexes # first output\n",
        "#   # build a probability distribution\n",
        "#   # give all chars weights of 1\n",
        "#   # and give the separator token a weight of 0.005\n",
        "#   # then divide by the sum to normalize and make\n",
        "#   # the probabilities sum up to 1\n",
        "  # prob_1 = np.float32(np.array(indexes) == 1)*0.005\n",
        "  # prob_2 = np.float32(np.array(indexes) != 1)\n",
        "  # prob = prob_1 + prob_2\n",
        "#   prob = prob / np.sum(prob)\n",
        "#   indices_to_mask = np.random.choice(np.arange(128), size=(19,), replace=False, p=prob)\n",
        "#   one_hot_mask = tf.cast(tf.reduce_sum(tf.one_hot(indices_to_mask, 128), axis=0), dtype=tf.int32).numpy() # second output\n",
        "\n",
        "#   y_masked = np.array(y_true.copy())\n",
        "\n",
        "#   y_masked[indices_to_mask] = 2\n",
        "#   y_masked = y_masked.tolist()\n",
        "\n",
        "#   return y_true, y_masked, one_hot_mask.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(indexes):\n",
        "  '''\n",
        "  Args: list of characters in the form of indexes\n",
        "  Returns: gold labels, masked labels, mask\n",
        "  '''\n",
        "  y_true = indexes # first output\n",
        "  indexes_len = len(indexes)\n",
        "\n",
        "  prob_1 = np.float32(np.array(indexes) == 1) * 0\n",
        "  prob_2 = np.float32(np.array(indexes) != 1)\n",
        "  prob = prob_1 + prob_2\n",
        "  prob = prob / np.sum(prob)\n",
        "\n",
        "  if indexes_len >= 5 and indexes_len <= 7:\n",
        "    indices_to_mask = np.random.choice(np.arange(indexes_len), size=(1,), replace=False, p=prob)\n",
        "    one_hot_mask = tf.cast(tf.reduce_sum(tf.one_hot(indices_to_mask, indexes_len), axis=0), dtype=tf.int32).numpy() # second output\n",
        "    y_masked = np.array(y_true.copy())\n",
        "    y_masked[indices_to_mask] = 2\n",
        "    y_masked = y_masked.tolist()\n",
        "\n",
        "  elif indexes_len > 5 and indexes_len <= 8:\n",
        "    indices_to_mask = np.random.choice(np.arange(indexes_len), size=(2,), replace=False, p=prob)\n",
        "    one_hot_mask = tf.cast(tf.reduce_sum(tf.one_hot(indices_to_mask, indexes_len), axis=0), dtype=tf.int32).numpy() # second output\n",
        "    y_masked = np.array(y_true.copy())\n",
        "    y_masked[indices_to_mask] = 2\n",
        "    y_masked = y_masked.tolist()\n",
        "\n",
        "  elif indexes_len > 8 and indexes_len <= 13:\n",
        "    indices_to_mask = np.random.choice(np.arange(indexes_len), size=(3,), replace=False, p=prob)\n",
        "    one_hot_mask = tf.cast(tf.reduce_sum(tf.one_hot(indices_to_mask, indexes_len), axis=0), dtype=tf.int32).numpy() # second output\n",
        "    y_masked = np.array(y_true.copy())\n",
        "    y_masked[indices_to_mask] = 2\n",
        "    y_masked = y_masked.tolist()\n",
        "\n",
        "  elif indexes_len > 13:\n",
        "    indices_to_mask = np.random.choice(np.arange(indexes_len), size=(4,), replace=False, p=prob)\n",
        "    one_hot_mask = tf.cast(tf.reduce_sum(tf.one_hot(indices_to_mask, indexes_len), axis=0), dtype=tf.int32).numpy() # second output\n",
        "    y_masked = np.array(y_true.copy())\n",
        "    y_masked[indices_to_mask] = 2\n",
        "    y_masked = y_masked.tolist()\n",
        "\n",
        "  return y_true, y_masked, one_hot_mask.tolist()"
      ],
      "metadata": {
        "id": "6EGdAJx5d2e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DRFfH09BUFO",
        "outputId": "9f64e6f3-65ab-4ff7-81ec-8a4b2ff4f1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1021455/1021455 [07:50<00:00, 2172.86it/s]\n",
            "100%|██████████| 56747/56747 [00:25<00:00, 2212.31it/s]\n",
            "100%|██████████| 56748/56748 [00:25<00:00, 2200.99it/s]\n"
          ]
        }
      ],
      "source": [
        "# construct the train and valid\n",
        "# true labels, masked labels, and masks\n",
        "\n",
        "train_true_labels = [] # true labels list\n",
        "train_masked_labels = [] # masked labels list\n",
        "train_masks = [] # masks\n",
        "# iterate over the training dataset\n",
        "for indexed_example in tqdm(train_data):\n",
        "  y_true, y_masked, one_hot_mask = prepare_data(indexed_example)\n",
        "  train_true_labels.append(y_true)\n",
        "  train_masked_labels.append(y_masked)\n",
        "  train_masks.append(one_hot_mask)\n",
        "\n",
        "valid_true_labels = [] # true labels list\n",
        "valid_masked_labels = [] # masked labels list\n",
        "valid_masks = [] # masks\n",
        "# iterate over the validation dataset\n",
        "for indexed_example in tqdm(valid_data):\n",
        "  y_true, y_masked, one_hot_mask = prepare_data(indexed_example)\n",
        "  valid_true_labels.append(y_true)\n",
        "  valid_masked_labels.append(y_masked)\n",
        "  valid_masks.append(one_hot_mask) \n",
        "\n",
        "test_true_labels = [] # true labels list\n",
        "test_masked_labels = [] # masked labels list\n",
        "test_masks = [] # masks\n",
        "# iterate over the validation dataset\n",
        "for indexed_example in tqdm(test_data):\n",
        "  y_true, y_masked, one_hot_mask = prepare_data(indexed_example)\n",
        "  test_true_labels.append(y_true)\n",
        "  test_masked_labels.append(y_masked)\n",
        "  test_masks.append(one_hot_mask) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a the validation dataframe initialized with zeros\n",
        "train_zeros = np.zeros((len(train_true_labels), 3))\n",
        "train_df = pd.DataFrame(columns=['x', 'y', 'mask'], data=train_zeros)\n",
        "del train_zeros\n",
        "gc.collect()\n",
        "train_df = train_df.astype('str')\n",
        "\n",
        "# populate the dataframe with the true labels, masked labels, and masks\n",
        "for i in tqdm(range(len(train_masked_labels))):\n",
        "  train_df.iloc[i, 0] = train_masked_labels[i]\n",
        "  train_df.iloc[i, 1] = train_true_labels[i]\n",
        "  train_df.iloc[i, 2] = train_masks[i]\n",
        "\n",
        "# write the dataframe to a CSV file\n",
        "train_df.to_csv('final_train_df.csv', index=False)\n",
        "del train_df\n",
        "gc.collect()\n",
        "# move valid_df.csv to drive\n",
        "!mv /content/final_train_df.csv /content/drive/MyDrive/NLP_Course"
      ],
      "metadata": {
        "id": "SjEVKK1VkLq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a the validation dataframe initialized with zeros\n",
        "valid_zeros = np.zeros((len(valid_true_labels), 3))\n",
        "valid_df = pd.DataFrame(columns=['x', 'y', 'mask'], data=valid_zeros)\n",
        "del valid_zeros\n",
        "gc.collect()\n",
        "valid_df = valid_df.astype('str')\n",
        "\n",
        "# populate the dataframe with the true labels, masked labels, and masks\n",
        "for i in tqdm(range(len(valid_masked_labels))):\n",
        "  valid_df.iloc[i, 0] = valid_masked_labels[i]\n",
        "  valid_df.iloc[i, 1] = valid_true_labels[i]\n",
        "  valid_df.iloc[i, 2] = valid_masks[i]\n",
        "\n",
        "# write the dataframe to a CSV file\n",
        "valid_df.to_csv('final_valid_df.csv', index=False)\n",
        "del valid_df\n",
        "gc.collect()\n",
        "# move valid_df.csv to drive\n",
        "!mv /content/final_valid_df.csv /content/drive/MyDrive/NLP_Course"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fouAUP86llb-",
        "outputId": "ab5a6cbf-7051-4439-cfc4-fa784c5e2649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56747/56747 [00:10<00:00, 5499.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_true_labels = [] # true labels list\n",
        "test_masked_labels = [] # masked labels list\n",
        "test_masks = [] # masks\n",
        "# iterate over the validation dataset\n",
        "for indexed_example in tqdm(test_data):\n",
        "  y_true, y_masked, one_hot_mask = prepare_data(indexed_example)\n",
        "  test_true_labels.append(y_true)\n",
        "  test_masked_labels.append(y_masked)\n",
        "  test_masks.append(one_hot_mask) \n",
        "\n",
        "# construct a the test dataframe initialized with zeros\n",
        "test_zeros = np.zeros((len(test_true_labels), 3))\n",
        "test_df = pd.DataFrame(columns=['x', 'y', 'mask'], data=test_zeros)\n",
        "del test_zeros\n",
        "gc.collect()\n",
        "test_df = test_df.astype('str')\n",
        "\n",
        "# populate the dataframe with the true labels, masked labels, and masks\n",
        "for i in tqdm(range(len(test_masked_labels))):\n",
        "  test_df.iloc[i, 0] = test_masked_labels[i]\n",
        "  test_df.iloc[i, 1] = test_true_labels[i]\n",
        "  test_df.iloc[i, 2] = test_masks[i]\n",
        "\n",
        "# write the dataframe to a CSV file\n",
        "test_df.to_csv('final_test_df.csv', index=False)\n",
        "del test_df\n",
        "gc.collect()\n",
        "# move test_df.csv to drive\n",
        "!mv /content/final_test_df.csv /content/drive/MyDrive/NLP_Course"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejZpB18ls2v",
        "outputId": "dc0504ac-3516-4663-ef87-2f2e78a017bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56748/56748 [00:26<00:00, 2140.93it/s]\n",
            "100%|██████████| 56748/56748 [00:10<00:00, 5495.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9aCaZwmviVS",
        "outputId": "6e0677d3-6354-4e1e-c40d-bfc4a9f0c3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 576728/576728 [04:51<00:00, 1981.37it/s]\n",
            "100%|██████████| 576728/576728 [01:58<00:00, 4853.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 576728/576728 [04:48<00:00, 1999.53it/s]\n",
            "100%|██████████| 576728/576728 [01:56<00:00, 4934.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 576728/576728 [04:54<00:00, 1960.59it/s]\n",
            "100%|██████████| 576728/576728 [02:02<00:00, 4715.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 576728/576728 [04:53<00:00, 1967.58it/s]\n",
            "100%|██████████| 576728/576728 [02:05<00:00, 4601.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 576728/576728 [05:03<00:00, 1898.87it/s]\n",
            "100%|██████████| 576728/576728 [01:56<00:00, 4935.55it/s]\n"
          ]
        }
      ],
      "source": [
        "# chunks = 5\n",
        "# for chunk in range(chunks):\n",
        "#   print(f'Processing chunk {chunk+1}')\n",
        "#   # construct the train and valid\n",
        "#   # true labels, masked labels, and masks\n",
        "#   train_chuck = train_data[chunk*(len(train_data)//5):(len(train_data)//5)*(chunk+1)]\n",
        "#   train_true_labels = [] # true labels list\n",
        "#   train_masked_labels = [] # masked labels list\n",
        "#   train_masks = [] # masks\n",
        "#   # iterate over the training dataset\n",
        "#   for indexed_example in tqdm(train_chuck):\n",
        "#     y_true, y_masked, one_hot_mask = prepare_data(indexed_example)\n",
        "#     train_true_labels.append(y_true)\n",
        "#     train_masked_labels.append(y_masked)\n",
        "#     train_masks.append(one_hot_mask)\n",
        "\n",
        "#   # construct a the training dataframe initialized with zeros\n",
        "#   train_zeros = np.zeros((len(train_true_labels), 3))\n",
        "#   train_df = pd.DataFrame(columns=['x', 'y', 'mask'], data=train_zeros)\n",
        "#   del train_zeros\n",
        "#   gc.collect()\n",
        "#   train_df = train_df.astype('str')\n",
        "\n",
        "#   # populate the dataframe with the true labels, masked labels, and masks\n",
        "#   for i in tqdm(range(len(train_masked_labels))):\n",
        "#     train_df.iloc[i, 0] = train_masked_labels[i]\n",
        "#     train_df.iloc[i, 1] = train_true_labels[i]\n",
        "#     train_df.iloc[i, 2] = train_masks[i]\n",
        "\n",
        "#   # write the dataframe to a CSV file\n",
        "#   train_df.to_csv(f'new_train_df_chunk{chunk+1}.csv', index=False)\n",
        "#   del train_df, train_true_labels, train_masked_labels, train_masks\n",
        "#   gc.collect()\n",
        "# # move train_df.csv to drive\n",
        "# # !mv /content/new_train_df.csv /content/drive/MyDrive/NLP_Course"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # move train_df.csv to drive\n",
        "# !mv /content/new* /content/drive/MyDrive/NLP_Course"
      ],
      "metadata": {
        "id": "UvNAgyqunI8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl6WGRPq1Up2",
        "outputId": "7211a854-2f6f-4b9b-a461-0e31bd5c13fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160202/160202 [01:29<00:00, 1794.45it/s]\n",
            "100%|██████████| 160202/160202 [00:32<00:00, 4972.76it/s]\n"
          ]
        }
      ],
      "source": [
        "# valid_true_labels = [] # true labels list\n",
        "# valid_masked_labels = [] # masked labels list\n",
        "# valid_masks = [] # masks\n",
        "# # iterate over the validation dataset\n",
        "# for indexed_example in tqdm(valid_data):\n",
        "#   y_true, y_masked, one_hot_mask = prepare_data(indexed_example)\n",
        "#   valid_true_labels.append(y_true)\n",
        "#   valid_masked_labels.append(y_masked)\n",
        "#   valid_masks.append(one_hot_mask)\n",
        "\n",
        "# # construct a the validation dataframe initialized with zeros\n",
        "# valid_zeros = np.zeros((len(valid_true_labels), 3))\n",
        "# valid_df = pd.DataFrame(columns=['x', 'y', 'mask'], data=valid_zeros)\n",
        "# del valid_zeros\n",
        "# gc.collect()\n",
        "# valid_df = valid_df.astype('str')\n",
        "\n",
        "# # populate the dataframe with the true labels, masked labels, and masks\n",
        "# for i in tqdm(range(len(valid_masked_labels))):\n",
        "#   valid_df.iloc[i, 0] = valid_masked_labels[i]\n",
        "#   valid_df.iloc[i, 1] = valid_true_labels[i]\n",
        "#   valid_df.iloc[i, 2] = valid_masks[i]\n",
        "\n",
        "# # write the dataframe to a CSV file\n",
        "# valid_df.to_csv('new_valid_df.csv', index=False)\n",
        "# del valid_df\n",
        "# gc.collect()\n",
        "# # move valid_df.csv to drive\n",
        "# !mv /content/new_valid_df.csv /content/drive/MyDrive/NLP_Course"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_true_labels = [] # true labels list\n",
        "# test_masked_labels = [] # masked labels list\n",
        "# test_masks = [] # masks\n",
        "# # iterate over the validation dataset\n",
        "# for indexed_example in tqdm(test_data):\n",
        "#   y_true, y_masked, one_hot_mask = prepare_data(indexed_example)\n",
        "#   test_true_labels.append(y_true)\n",
        "#   test_masked_labels.append(y_masked)\n",
        "#   test_masks.append(one_hot_mask) \n",
        "\n",
        "# # construct a the test dataframe initialized with zeros\n",
        "# test_zeros = np.zeros((len(test_true_labels), 3))\n",
        "# test_df = pd.DataFrame(columns=['x', 'y', 'mask'], data=test_zeros)\n",
        "# del test_zeros\n",
        "# gc.collect()\n",
        "# test_df = test_df.astype('str')\n",
        "\n",
        "# # populate the dataframe with the true labels, masked labels, and masks\n",
        "# for i in tqdm(range(len(test_masked_labels))):\n",
        "#   test_df.iloc[i, 0] = test_masked_labels[i]\n",
        "#   test_df.iloc[i, 1] = test_true_labels[i]\n",
        "#   test_df.iloc[i, 2] = test_masks[i]\n",
        "\n",
        "# # write the dataframe to a CSV file\n",
        "# test_df.to_csv('new_test_df.csv', index=False)\n",
        "# del test_df\n",
        "# gc.collect()\n",
        "# # move test_df.csv to drive\n",
        "# !mv /content/new_test_df.csv /content/drive/MyDrive/NLP_Course"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0roa6GyWG4Z",
        "outputId": "d21e4f9b-3b9b-481b-c06e-21bfdf8dd263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160203/160203 [01:20<00:00, 1992.56it/s]\n",
            "100%|██████████| 160203/160203 [00:32<00:00, 4977.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIqVhh8ZBul8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "01 - Final Pre-training Data Preparation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
